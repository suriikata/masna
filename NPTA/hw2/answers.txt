TASK 1.

The first dataset was condensed to 11 features to simplify the analysis, and what is interesting is that most of these variables are measured on continious scale. After evaluating the variable summary, the missing values were removed, leading to a final sample of 3466 units. Next, the normality assessment via histogram, boxplot, and density plot demonstrated that all numeric variables exhibit a strong right skewness, with rate and altitude having a relatively even distribution of values on the right-hand side of the histograms and density plots. However, the remaining variables revealed extreme values, which result in a prominent long tail in histogram as well as in density plot. In order to obtain meaningful results, it will be necessary to exclude these extreme values from these five variables to achieve effective clustering. The Shapiro-Wilk and Anderson-Darling normality tests show statistically significant p-values (p < 0.05) for all numerical variables, meaning that the null hypothesis of normality is rejected for each variable. Upon examining the numerical variables with corrplot, it became clear that most of the variables were not correlated, but I discovered a strong relationship between the rate of the ski resort and the number of easy and intermediate tracks, which led me to choose these three variables for further analysis. 

When considering cluster algorithm setups for the first dataset, which is characterized by continuous variables, non-normal distribution, and the presence of significant outliers, the most promising method appears to be median-based clustering. This is due to the fact that this method uses the median to determine the cluster centers, which is less sensitive to extreme values than the mean. Another viable clustering option for the first dataset is density estimation via kernel density estimators, as this method can effectively handle noise and outliers, typically excluding them from the clusters due to their low density estimates. Conversely, binary MCMC coupled-Metropolis clustering is not a suitable choice, given that our variables are predominantly continuous. If necessary and hypothethically, it could be possible to convert the variables into dummy variables or apply a threshold value to assign 0 or 1 to each variable.

The second dataset comprises health information data, which consists mostly of binary and nominal variables, the latter are intended to be converted into binary form. There are also two continuous variables included in the dataset: age and average glucose level. The distribution of age is approximately normal, while the distribution of average glucose level is skewed to the right.In this case, the binary MCMC coupled-Metropolis method appears to be the most appropriate clustering technique. This is because it is designed to consider the binary nature of the data, resulting in more efficient clustering results, as the name suggests. In contrast, the median and probability density methods are not applicable, as they assume that the variables are continuous. Instead, the data can be clustered using the average glucose variable, while the normally distributed age variable is not suitable.
	

TASK 2. 

During my analysis using gmedian function, I observed that reducing the number of clusters resulted in less differentiation between the clusters based on the selected 3 variables, but the advantage of this was observed in that the groups have become comparable in size. Ultimately, I decided to use 6 clusters, because the differences between clusters were apparent and varied between the clusters, and secondly because I assumed that the clustering would be capable of separating ski resorts by continent, of which there were 6. Hence, before interpreting the gmedian output I decided to explore the distribution of these three variables across continents using boxplot. A trend was be observed that ski resorts with a higher rating typically have more trails with easy and medium difficulty, particularly Ocenia, South and North America, as well as Asia. The distribution of Europe was found to be interesting, as it has a significantly lower average rating compared to the other continents, but with a lot of outliers, which are also visible in the other two plots. Looking at the clusters obtained, results indicate that the two smaller clusters have a high average rating (e.g. 2 and 4) and a wider range of easy and challenging routes compared to those with a lower rating.  I assume that this clusters contain American or Asian ski resorts, based on prior exploratory analysis. Cluster 5 (and possibly 1) has the lowest rating and a smaller number of both types of tracks, with numerous outliers. Therefore, it is likely that these clusters represent European skiers, for whom these characteristics have been verified.


TASK 3.

The first attempt of clustering using MCMC method, was to generate the parameters we discussed in the lectures, increasing the number of iterations to 175 and the heat to 75 as recommended. The result revealed that there are 6 clusters with a probability of 1, which is an excellent outcome. However, the swap acceptance rate was much lower than the expected optimal value (1 %). Upon further investigation using a barplot with Stephen benchmark, it became apparent that the distribution of the clusters are remarkably unevenly distributed. Specifically, at least two clusters have substantially lower values (14 and 69) compared to the largest cluster, which consists of roughly 2000 units. That is why, in the next step I decided to limit the upper limit of numbers of clusters to 3, hoping that the swap acceptance rate will improve with this. The analysis revealed that the current approach was insufficient, but despite this, the probability of dividing the elements into three clusters was 1, as anticipated. Additionally, the number of data points in the resulting clusters was also larger. In the following, it was necessary to adjust the parameters related to the number of chains and heats used. Upon manipulating these parameters, it became evident that increasing the temperature of the chain was required, leading to a significant improvement in the swap acceptance rate. Given the significant time consuming involved, I opted to stop the analysis once the swap acceptance rate had reached 26%, which is still considered to be a satisfactory outcome. The results of the analysis indicate a high probability of 1 clustering the units into three groups.

TASK 4.

During the initial optimization process using kernel algorithm, the Epanechnikov, triangle, and biweight optimization processes were evaluated and compared. The results indicated that the triangle kernel optimization process provided the most promising results. This was likely due to the fact that the triangle kernel function balanced the trade-off between robustness and smoothness in the estimated density, making it more effective in capturing important features in the data, for instance the peak, while also being less sensitive to outliers.

Upon analyzing the results of the median-based linear model, it is evident that the variable "easy" is statistically significant at a level of less than 1%. This indicates that a one-kilometer increase in easier tracks is associated with an increase of 0.25 units in ratings. Additionally, the variable "intermediate" also appears to be significant at a 1% level, implying that a one-kilometer increase in intermediate tracks is linked with an increase of 0.27 units in ratings. In both scenarios, we observe a positive and consistent correlation between the length of the track and the ratings of ski resorts. This is evidenced by the positively sloping lines on the graphs. However, the results also reveal a high residual standard error of approximately 0.29 in both cases. This indicates that there is a significant amount of variation in the ratings variable that is not accounted for by the track length. This is demonstrated by a 'cloud' of values around a rating value of 3 on both plots.

Finally, I conducted GAM modeling with and without smoothing parameters. In both instances, the coefficient estimates for "easy" and "intermediate" were statistically significant at a level of 1% and positively associated with ski resort ratings. In the first model, which did not incorporate smoothing, a one-kilometer increase in "easy" and "intermediate" tracks was linked to an increase of 0.12 and 0.15 units in ratings, respectively and holding the rest of the factors contant. Although the R-squared and GCV values remained unchanged even after incorporating smoothing, the graphical representation of the results suggests that smoother values perform better. Despite this, when the chi-square of differences between the models was calculated, it was found that there was no significant difference between normal and smoothed values. That said, in my opinion the GAM (with smothing) appears to provide the most promising results, when compared with the rest of the models. 


TASK 5. 

In the first model, I used the average glucose levels of the participants as the outcome variable, while taking into account selected demographic and health factors as predictors. Prior to modeling, I examined the distribution of the predictor variable and observed that it had a positive skewness with heavier tail and a higher peak than a normal distribution. This suggested that the variable may follow an inverse Gaussian distribution. Based on the model output, we can state with 99% confidence that an individual's average glucose level is significantly associated by their gender, marital status, the presence of hypertension, heart disease, stroke in the population. All of these factors have positive coefficients, suggesting a positive association in predicting glucose levels. Since all the independent variables are on the same scale, we can compare the coefficients to determine which variable has the greatest influence on glucose levels. Our analysis reveals that the presence of heart disease has the strongest impact on glucose levels among all the variables considered, indicating that for every one-unit increase in the presence of heart disease, the average glucose level is expected to increase by 0.18 units, all other variables being held constant. In terms of reference group, the expected average glucose levels of around is to be expected for single males with no hypertension, heart disease and probability of stroke. Although these results are promising, I decided to optimize the distribution of the response variable further by removing values above 85% and using a kernel optimization algorithm. However, when I built the same model on the revised distribution, it performed worse. This is evident from the non-significant variables, although the intercept remained significant with a p-value of less than .001. further examination would be needed. 


In the second model, we used the same set of binary variables as in the first model to predict the probability of a stroke occurrence. Our analysis showed that out of the set of predictors, hypertension, heart disease, and marital status had a significant impact on predicting the stroke occurrence with a p-value of almost 0. Specifically, we found that heart disease had the greatest effect on the likelihood of a stroke. For every one unit increase in heart disease, the probability of a stroke occurrence increased by 1.14.
Given that all predictor variables are binary, the intercept in this generalized linear model can be interpreted as the expected log odds of having a stroke for a reference group. This reference group consists of individuals who have no hypertension, heart disease, and are not married. The calculated value for this intercept is -3.98. After analyzing each significant variable in the cross-table, we observed that in all three instances, the Chi-square value is statistically significant and that the rectangle of individuals with both positive values is relatively low. In fact, even when analyzing the frequency of the stroke occurrence, we noticed that the percentage of individuals with a probability of stroke is only 6%. To further validate this findings, it would be necessary to collect additional data from individuals who have a probability of having a stroke. This additional data could improve the robustness of the results and help identify more accurate predictors of heart stroke.









